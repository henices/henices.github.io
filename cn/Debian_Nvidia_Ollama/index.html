<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Nvidia 驱动安装和 Ollama 的使用 | 安全代码</title><meta name="author" content="曼福吉"><meta name="copyright" content="曼福吉"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="根据同事反馈，高版本的 NVIDIA 驱动兼容性有问题，需要安装 Nvidia 驱动 525.147.05 ，过程中可能需要升级内核。 安装 Nvidia 驱动查看 Debian 上显卡安装情况。 12lspci -nn | egrep -i &quot;3d|display|vga&quot;  01:00.0 VGA compatible controller [0300]: NVIDIA C">
<meta property="og:type" content="article">
<meta property="og:title" content="Nvidia 驱动安装和 Ollama 的使用">
<meta property="og:url" content="https://usmacd.com/cn/Debian_Nvidia_Ollama/index.html">
<meta property="og:site_name" content="安全代码">
<meta property="og:description" content="根据同事反馈，高版本的 NVIDIA 驱动兼容性有问题，需要安装 Nvidia 驱动 525.147.05 ，过程中可能需要升级内核。 安装 Nvidia 驱动查看 Debian 上显卡安装情况。 12lspci -nn | egrep -i &quot;3d|display|vga&quot;  01:00.0 VGA compatible controller [0300]: NVIDIA C">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://images.unsplash.com/photo-1646026371686-79950ceb6daa?w=640">
<meta property="article:published_time" content="2024-07-25T16:00:00.000Z">
<meta property="article:modified_time" content="2025-01-01T16:00:00.000Z">
<meta property="article:author" content="曼福吉">
<meta property="article:tag" content="ai">
<meta property="article:tag" content="linux">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://images.unsplash.com/photo-1646026371686-79950ceb6daa?w=640"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Nvidia 驱动安装和 Ollama 的使用",
  "url": "https://usmacd.com/cn/Debian_Nvidia_Ollama/",
  "image": "https://images.unsplash.com/photo-1646026371686-79950ceb6daa?w=640",
  "datePublished": "2024-07-25T16:00:00.000Z",
  "dateModified": "2025-01-01T16:00:00.000Z",
  "author": [
    {
      "@type": "Person",
      "name": "曼福吉",
      "url": "https://usmacd.com"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://usmacd.com/cn/Debian_Nvidia_Ollama/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css?v=5.5.3"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'undefined')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'undefined')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"pagination":{"enable":false,"hitsPerPage":8},"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":false,"highlightHeightLimit":false,"highlightFullpage":true,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Nvidia 驱动安装和 Ollama 的使用',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="/self/atom-one-light.css"><link rel="stylesheet" href="/self/nord.css"><meta name="generator" content="Hexo 8.1.0"><link rel="alternate" href="/atom.xml" title="安全代码" type="application/atom+xml">
<link rel="alternate" href="/rss.xml" title="安全代码" type="application/rss+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/images/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">168</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">17</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/thoughts"><i class="fa-fw fas fa-cloud"></i><span> 想法</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/sitemap.xml"><span> sitemap</span></a></div><div class="menus_item"><a class="site-page" href="/random.html"><span> random</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://images.unsplash.com/photo-1646026371686-79950ceb6daa?w=640);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">安全代码</span></a><a class="nav-page-title" href="/"><span class="site-name">Nvidia 驱动安装和 Ollama 的使用</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/thoughts"><i class="fa-fw fas fa-cloud"></i><span> 想法</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/sitemap.xml"><span> sitemap</span></a></div><div class="menus_item"><a class="site-page" href="/random.html"><span> random</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Nvidia 驱动安装和 Ollama 的使用</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-07-25T16:00:00.000Z" title="发表于 2024-07-26 00:00:00">2024-07-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-01-01T16:00:00.000Z" title="更新于 2025-01-02 00:00:00">2025-01-02</time></span></div><div class="meta-secondline"></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p>根据同事反馈，高版本的 NVIDIA 驱动兼容性有问题，需要安装 Nvidia 驱动 525.147.05 ，过程中可能需要升级内核。</p>
<h2 id="安装-Nvidia-驱动"><a href="#安装-Nvidia-驱动" class="headerlink" title="安装 Nvidia 驱动"></a>安装 Nvidia 驱动</h2><p>查看 Debian 上显卡安装情况。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">lspci -nn | egrep -i &quot;3d|display|vga&quot;  <br>01:00.0 VGA compatible controller [0300]: NVIDIA Corporation AD102 [GeForce RTX 4090] [10de:2684] (rev a1)<br></code></pre></td></tr></table></figure>

<p>查看驱动安装具体的情况。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">lsmod | grep nouveau  <br>nouveau              2433024  0  <br>mxm_wmi                16384  1 nouveau  <br>i2c_algo_bit           16384  1 nouveau  <br>drm_display_helper    184320  1 nouveau  <br>drm_ttm_helper         16384  1 nouveau  <br>ttm                    94208  2 drm_ttm_helper,nouveau  <br>drm_kms_helper        204800  2 drm_display_helper,nouveau  <br>drm                   614400  5 drm_kms_helper,drm_display_helper,drm_ttm_helper,ttm,nouveau  <br>video                  65536  2 asus_wmi,nouveau  <br>wmi                    36864  5 video,asus_wmi,wmi_bmof,mxm_wmi,nouveau  <br>button                 24576  1 nouveau<br></code></pre></td></tr></table></figure>

<p>看来安装的是开源版本的驱动 <code>nouveau</code>，需要先禁用。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;blacklist nouveau&quot;</span> | <span class="hljs-built_in">sudo</span> <span class="hljs-built_in">tee</span> /etc/modprobe.d/nouveau-blacklist.conf<br><span class="hljs-built_in">sudo</span> update-initramfs -u<br><span class="hljs-built_in">sudo</span> update-grub<br><span class="hljs-built_in">sudo</span> reboot<br></code></pre></td></tr></table></figure>

<p>重启后，执行 <code>lsmod | grep nouveau</code>  发现已经返回为空了，成功禁用。</p>
<p>执行命令<code> sudo apt install nvidia-driver firmware-misc-nonfree</code> 安装 NVIDIA Proprietary Driver 报错。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">Consult /var/lib/dkms/nvidia-current/525.147.05/build/make.log for more information.  <br>dpkg: error processing package nvidia-kernel-dkms (--configure):  <br>installed nvidia-kernel-dkms package post-installation script subprocess returned error exit status 10  <br>dpkg: dependency problems prevent configuration of nvidia-driver:  <br>nvidia-driver depends on nvidia-kernel-dkms (= 525.147.05-4~deb12u1) | nvidia-kernel-525.147.05 | nvidia-open-kernel-525.147.05 | nvidia-open-kernel-525.147.05; however:  <br> Package nvidia-kernel-dkms is not configured yet.  <br> Package nvidia-kernel-525.147.05 is not installed.  <br> Package nvidia-kernel-dkms which provides nvidia-kernel-525.147.05 is not configured yet.  <br> Package nvidia-open-kernel-525.147.05 is not installed.  <br> Package nvidia-open-kernel-525.147.05 is not installed.  <br>  <br>dpkg: error processing package nvidia-driver (--configure):  <br>dependency problems - leaving unconfigured  <br>Processing triggers for libc-bin (2.36-9+deb12u4) ...  <br>Processing triggers for initramfs-tools (0.142) ...  <br>update-initramfs: Generating /boot/initrd.img-6.1.0-18-amd64  <br>Processing triggers for update-glx (1.2.2) ...  <br>Processing triggers for glx-alternative-nvidia (1.2.2) ...  <br>update-alternatives: using /usr/lib/nvidia to provide /usr/lib/glx (glx) in auto mode  <br>Processing triggers for glx-alternative-mesa (1.2.2) ...  <br>Processing triggers for libc-bin (2.36-9+deb12u4) ...  <br>Processing triggers for initramfs-tools (0.142) ...  <br>update-initramfs: Generating /boot/initrd.img-6.1.0-18-amd64  <br>Errors were encountered while processing:  <br>nvidia-kernel-dkms  <br>nvidia-driver  <br>E: Sub-process /usr/bin/dpkg returned an error code (1)<br></code></pre></td></tr></table></figure>

<p>确认 debian 版本 <code>lsb_release -a</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">No LSB modules are available.  <br>Distributor ID: Debian  <br>Description:    Debian GNU/Linux 12 (bookworm)  <br>Release:        12  <br>Codename:       bookworm<br></code></pre></td></tr></table></figure>

<p>根据 stackexchange 上的回答 ，安全升级 Debian 内核的方法是使用 backports 安装。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;deb http://deb.debian.org/debian bookworm-backports main&quot;</span> | <span class="hljs-built_in">sudo</span> <span class="hljs-built_in">tee</span> /etc/apt/sources.list.d/debian-backports.list<br><span class="hljs-built_in">sudo</span> apt update<br><span class="hljs-built_in">sudo</span> apt install -t bookworm-backports linux-image-amd64<br><span class="hljs-built_in">sudo</span> reboot<br></code></pre></td></tr></table></figure>

<p>重新启动后，执行 <code>uname -a</code>  发现内核已经成功升级了。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">uname -a                                                                          <br>Linux debian 6.7.12+bpo-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.7.12-1~bpo12+1 (2024-05-06) x86_64 GNU/Linux<br></code></pre></td></tr></table></figure>

<p>重新安装 NVIDIA Proprietary Driver <code>sudo apt install nvidia-driver firmware-misc-nonfree</code> ，这次没有报错了。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">nvidia-smi    <br><br>NVIDIA-SMI 525.147.05   Driver Version: 525.147.05   CUDA Version: 12.0<br></code></pre></td></tr></table></figure>

<p>NVIDIA Proprietary Driver 525 感觉有问题，过了一段时间后机器出现重启现象，dmesg 显示错误 <code>ACPI BIOS Error (bug)</code> 。 </p>
<p>上网搜索错误，有人反馈是 525 驱动问题（不确定）。Debain 系统 Nvidia 驱动有更新，执行 <code>apt upgrade</code> 后成功升级到 535 。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"># nvidia-smi<br><br>NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2<br></code></pre></td></tr></table></figure>

<p>升级 Nvidia 驱动到 535 后，暂未出现重启现象。</p>
<h2 id="安装-ollama"><a href="#安装-ollama" class="headerlink" title="安装 ollama"></a>安装 ollama</h2><p>执行下面的命令安装 Ollama</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">curl -fsSL https://ollama.com/install.sh | sh<br></code></pre></td></tr></table></figure>

<p>下载速度很慢，还是挂线路。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">export https_proxy=http://127.0.0.1:7890<br>export http_proxy=http://127.0.0.1:7890<br>curl -fsSL https://ollama.com/install.sh | sh<br></code></pre></td></tr></table></figure>

<p>挂上线路后，很快 Ollama 就安装成功了。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">&gt;&gt;&gt; Downloading ollama...  <br>######################################################################## 100.0%#=#=-#  #                                                                        <br>&gt;&gt;&gt; Installing ollama to /usr/local/bin...  <br>&gt;&gt;&gt; Creating ollama user...  <br>&gt;&gt;&gt; Adding ollama user to render group...  <br>&gt;&gt;&gt; Adding ollama user to video group...  <br>&gt;&gt;&gt; Adding current user to ollama group...  <br>&gt;&gt;&gt; Creating ollama systemd service...  <br>&gt;&gt;&gt; Enabling and starting ollama service...  <br>Created symlink /etc/systemd/system/default.target.wants/ollama.service → /etc/systemd/system/ollama.service.  <br>&gt;&gt;&gt; NVIDIA GPU installed.<br></code></pre></td></tr></table></figure>

<p>ollama 下载 llama3 8b 和 qwen2 7b 模型，执行下面的命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">ollama pull llama3<br>ollama pull qwen2:7b<br></code></pre></td></tr></table></figure>

<p>测试 llama3 模型，运行正常。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">ollama run llama3  <br>&gt;&gt;&gt; hi  <br>Hi! It&#x27;s nice to meet you. Is there something I can help you with or would you like to chat?<br></code></pre></td></tr></table></figure>
<h2 id="升级-Ollama"><a href="#升级-Ollama" class="headerlink" title="升级 Ollama"></a>升级 Ollama</h2><p>Ollama 0.3.0 支持通过 llama3.1 进行工具调用，有必要升级。参见 [4]</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-built_in">sudo</span> curl -L https://ollama.com/download/ollama-linux-amd64 -o /usr/local/bin/ollama<br><span class="hljs-built_in">sudo</span> <span class="hljs-built_in">chmod</span> +x /usr/local/bin/ollama<br></code></pre></td></tr></table></figure>

<p>升级完毕，需要重启 Ollama 服务。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-built_in">sudo</span> systemctl daemon-reload<br><span class="hljs-built_in">sudo</span> systemctl restart ollama<br></code></pre></td></tr></table></figure>

<p>新版本的 Ollama 已经不是一个单独的文件，而是一个 <code>tar.gz</code> 的压缩包。<br>tar.tgz 中包含了 ollama 运行需要的动态库，在升级前需要将这些动态库删除。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-built_in">sudo</span> <span class="hljs-built_in">rm</span> -rf /usr/lib/ollama<br><span class="hljs-built_in">sudo</span> <span class="hljs-built_in">rm</span> -rf /usr/local/lib/ollama<br><br>curl -L https://ollama.com/download/ollama-linux-amd64.tgz -o ollama-linux-amd64.tgz<br><span class="hljs-built_in">sudo</span> tar -C /usr/local -xzf ollama-linux-amd64.tgz<br></code></pre></td></tr></table></figure>

<p>执行完上述命令后，启动 ollama 检查版本确认是否升级成功。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sh">ollama serve<br>ollama -v<br></code></pre></td></tr></table></figure>

<h2 id="配置-Ollama"><a href="#配置-Ollama" class="headerlink" title="配置 Ollama"></a>配置 Ollama</h2><p>如果需要在浏览器插件（比如沉浸翻译）中调用 Ollama api，涉及 Cross-Origin 访问，需要修改 Ollama 配置。</p>
<p>官方文档提到了相关的设置 [5]，用 vim 直接修改 &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;ollama.service 中，添加下面内容：</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">Environment</span>=<span class="hljs-string">&quot;OLLAMA_HOST=*&quot;</span><br></code></pre></td></tr></table></figure>

<p>重启 Ollama 服务</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-built_in">sudo</span> systemctl daemon-reload<br><span class="hljs-built_in">sudo</span> systemctl restart ollama<br></code></pre></td></tr></table></figure>

<p>在远程主机上，查看 Ollama 端口侦听情况</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sh">apt install net-tools<br>netstat -antp | grep -i ollama  <br></code></pre></td></tr></table></figure>

<p>Ollama 默认侦听 127.0.0.1:11434</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">tcp        0      0 127.0.0.1:11434         0.0.0.0:*               LISTEN      50508/ollama<br></code></pre></td></tr></table></figure>

<p>利用 SSH 将远程主机 Ollama 侦听的端口 11434 转发到本地 127.0.0.1:11434</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">ssh -N -g -L 127.0.0.1:11434:127.0.0.1:11434 root@1.1.1.1  <span class="hljs-comment"># 将 1.1.1.1 替换成你的 ip</span><br></code></pre></td></tr></table></figure>
<h2 id="卸载-Ollama"><a href="#卸载-Ollama" class="headerlink" title="卸载 Ollama"></a>卸载 Ollama</h2><p>停止 Ollama 服务</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-built_in">sudo</span> systemctl stop ollama<br><span class="hljs-built_in">sudo</span> systemctl <span class="hljs-built_in">disable</span> ollama<br><span class="hljs-built_in">sudo</span> <span class="hljs-built_in">rm</span> /etc/systemd/system/ollama.service<br></code></pre></td></tr></table></figure>

<p>删除二进制文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">sudo rm $(which ollama)<br></code></pre></td></tr></table></figure>

<p>删除 Ollama 用户</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-built_in">sudo</span> <span class="hljs-built_in">rm</span> -r /usr/share/ollama<br><span class="hljs-built_in">sudo</span> userdel ollama<br><span class="hljs-built_in">sudo</span> groupdel ollama<br></code></pre></td></tr></table></figure>

<h2 id="Ollama-的使用"><a href="#Ollama-的使用" class="headerlink" title="Ollama 的使用"></a>Ollama 的使用</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sh">OLLAMA_ORIGINS=* OLLAMA_NUM_PARALLEL=16 ollama serve<br></code></pre></td></tr></table></figure>

<ul>
<li>OLLAMA_ORIGINS 跨域设置</li>
<li>OLLAMA_NUM_PARALLEL 支持的并行请求数量</li>
<li>OLLAMA_DEBUG 打印调试信息</li>
<li>OLLAMA_LLM_LIBRARY 支持下面的选项  rocm_v6 cpu cpu_avx cpu_avx2 cuda_v11 rocm_v5</li>
<li>OLLAMA_KEEP_ALIVE 模型在显存内加载的时间，默认为 5 分钟</li>
<li>OLLAMA_GPU_OVERHEAD 单独为每个 GPU 预留的 VRAM ，单位是字节</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/ollama/ollama/blob/main/envconfig/config.go">https://github.com/ollama/ollama/blob/main/envconfig/config.go</a></p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">AsMap</span><span class="hljs-params">()</span></span> <span class="hljs-keyword">map</span>[<span class="hljs-type">string</span>]EnvVar &#123;<br>	ret := <span class="hljs-keyword">map</span>[<span class="hljs-type">string</span>]EnvVar&#123;<br>		<span class="hljs-string">&quot;OLLAMA_DEBUG&quot;</span>:             &#123;<span class="hljs-string">&quot;OLLAMA_DEBUG&quot;</span>, Debug(), <span class="hljs-string">&quot;Show additional debug information (e.g. OLLAMA_DEBUG=1)&quot;</span>&#125;,<br>		<span class="hljs-string">&quot;OLLAMA_FLASH_ATTENTION&quot;</span>:   &#123;<span class="hljs-string">&quot;OLLAMA_FLASH_ATTENTION&quot;</span>, FlashAttention(), <span class="hljs-string">&quot;Enabled flash attention&quot;</span>&#125;,<br>		<span class="hljs-string">&quot;OLLAMA_HOST&quot;</span>:              &#123;<span class="hljs-string">&quot;OLLAMA_HOST&quot;</span>, Host(), <span class="hljs-string">&quot;IP Address for the ollama server (default 127.0.0.1:11434)&quot;</span>&#125;,<br>		<span class="hljs-string">&quot;OLLAMA_KEEP_ALIVE&quot;</span>:        &#123;<span class="hljs-string">&quot;OLLAMA_KEEP_ALIVE&quot;</span>, KeepAlive(), <span class="hljs-string">&quot;The duration that models stay loaded in memory (default \&quot;5m\&quot;)&quot;</span>&#125;,<br>		<span class="hljs-string">&quot;OLLAMA_LLM_LIBRARY&quot;</span>:       &#123;<span class="hljs-string">&quot;OLLAMA_LLM_LIBRARY&quot;</span>, LLMLibrary(), <span class="hljs-string">&quot;Set LLM library to bypass autodetection&quot;</span>&#125;,<br>		<span class="hljs-string">&quot;OLLAMA_MAX_LOADED_MODELS&quot;</span>: &#123;<span class="hljs-string">&quot;OLLAMA_MAX_LOADED_MODELS&quot;</span>, MaxRunners(), <span class="hljs-string">&quot;Maximum number of loaded models per GPU&quot;</span>&#125;,<br>		<span class="hljs-string">&quot;OLLAMA_MAX_QUEUE&quot;</span>:         &#123;<span class="hljs-string">&quot;OLLAMA_MAX_QUEUE&quot;</span>, MaxQueue(), <span class="hljs-string">&quot;Maximum number of queued requests&quot;</span>&#125;,<br>		<span class="hljs-string">&quot;OLLAMA_MODELS&quot;</span>:            &#123;<span class="hljs-string">&quot;OLLAMA_MODELS&quot;</span>, Models(), <span class="hljs-string">&quot;The path to the models directory&quot;</span>&#125;,<br>		<span class="hljs-string">&quot;OLLAMA_NOHISTORY&quot;</span>:         &#123;<span class="hljs-string">&quot;OLLAMA_NOHISTORY&quot;</span>, NoHistory(), <span class="hljs-string">&quot;Do not preserve readline history&quot;</span>&#125;,<br>		<span class="hljs-string">&quot;OLLAMA_NOPRUNE&quot;</span>:           &#123;<span class="hljs-string">&quot;OLLAMA_NOPRUNE&quot;</span>, NoPrune(), <span class="hljs-string">&quot;Do not prune model blobs on startup&quot;</span>&#125;,<br>		<span class="hljs-string">&quot;OLLAMA_NUM_PARALLEL&quot;</span>:      &#123;<span class="hljs-string">&quot;OLLAMA_NUM_PARALLEL&quot;</span>, NumParallel(), <span class="hljs-string">&quot;Maximum number of parallel requests&quot;</span>&#125;,<br>		<span class="hljs-string">&quot;OLLAMA_ORIGINS&quot;</span>:           &#123;<span class="hljs-string">&quot;OLLAMA_ORIGINS&quot;</span>, Origins(), <span class="hljs-string">&quot;A comma separated list of allowed origins&quot;</span>&#125;,<br>		<span class="hljs-string">&quot;OLLAMA_RUNNERS_DIR&quot;</span>:       &#123;<span class="hljs-string">&quot;OLLAMA_RUNNERS_DIR&quot;</span>, RunnersDir(), <span class="hljs-string">&quot;Location for runners&quot;</span>&#125;,<br>		<span class="hljs-string">&quot;OLLAMA_SCHED_SPREAD&quot;</span>:      &#123;<span class="hljs-string">&quot;OLLAMA_SCHED_SPREAD&quot;</span>, SchedSpread(), <span class="hljs-string">&quot;Always schedule model across all GPUs&quot;</span>&#125;,<br>		<span class="hljs-string">&quot;OLLAMA_TMPDIR&quot;</span>:            &#123;<span class="hljs-string">&quot;OLLAMA_TMPDIR&quot;</span>, TmpDir(), <span class="hljs-string">&quot;Location for temporary files&quot;</span>&#125;,<br>	&#125;<br>	<span class="hljs-keyword">if</span> runtime.GOOS != <span class="hljs-string">&quot;darwin&quot;</span> &#123;<br>		ret[<span class="hljs-string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = EnvVar&#123;<span class="hljs-string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>, CudaVisibleDevices(), <span class="hljs-string">&quot;Set which NVIDIA devices are visible&quot;</span>&#125;<br>		ret[<span class="hljs-string">&quot;HIP_VISIBLE_DEVICES&quot;</span>] = EnvVar&#123;<span class="hljs-string">&quot;HIP_VISIBLE_DEVICES&quot;</span>, HipVisibleDevices(), <span class="hljs-string">&quot;Set which AMD devices are visible&quot;</span>&#125;<br>		ret[<span class="hljs-string">&quot;ROCR_VISIBLE_DEVICES&quot;</span>] = EnvVar&#123;<span class="hljs-string">&quot;ROCR_VISIBLE_DEVICES&quot;</span>, RocrVisibleDevices(), <span class="hljs-string">&quot;Set which AMD devices are visible&quot;</span>&#125;<br>		ret[<span class="hljs-string">&quot;GPU_DEVICE_ORDINAL&quot;</span>] = EnvVar&#123;<span class="hljs-string">&quot;GPU_DEVICE_ORDINAL&quot;</span>, GpuDeviceOrdinal(), <span class="hljs-string">&quot;Set which AMD devices are visible&quot;</span>&#125;<br>		ret[<span class="hljs-string">&quot;HSA_OVERRIDE_GFX_VERSION&quot;</span>] = EnvVar&#123;<span class="hljs-string">&quot;HSA_OVERRIDE_GFX_VERSION&quot;</span>, HsaOverrideGfxVersion(), <span class="hljs-string">&quot;Override the gfx used for all detected AMD GPUs&quot;</span>&#125;<br>		ret[<span class="hljs-string">&quot;OLLAMA_INTEL_GPU&quot;</span>] = EnvVar&#123;<span class="hljs-string">&quot;OLLAMA_INTEL_GPU&quot;</span>, IntelGPU(), <span class="hljs-string">&quot;Enable experimental Intel GPU detection&quot;</span>&#125;<br>	&#125;<br>	<span class="hljs-keyword">return</span> ret<br>&#125;<br></code></pre></td></tr></table></figure>

<p>可以使用 <code>ollama serve -h</code> 查看官方明确支持的环境变量。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs sh">ollama serve -h  <br>Start ollama  <br>  <br>Usage:  <br> ollama serve [flags]  <br>  <br>Aliases:  <br> serve, start  <br>  <br>Flags:  <br> -h, --<span class="hljs-built_in">help</span>   <span class="hljs-built_in">help</span> <span class="hljs-keyword">for</span> serve  <br>  <br>Environment Variables:  <br>     OLLAMA_DEBUG               Show additional debug information (e.g. OLLAMA_DEBUG=1)  <br>     OLLAMA_HOST                IP Address <span class="hljs-keyword">for</span> the ollama server (default 127.0.0.1:11434)  <br>     OLLAMA_KEEP_ALIVE          The duration that models stay loaded <span class="hljs-keyword">in</span> memory (default <span class="hljs-string">&quot;5m&quot;</span>)  <br>     OLLAMA_MAX_LOADED_MODELS   Maximum number of loaded models per GPU  <br>     OLLAMA_MAX_QUEUE           Maximum number of queued requests  <br>     OLLAMA_MODELS              The path to the models directory  <br>     OLLAMA_NUM_PARALLEL        Maximum number of parallel requests  <br>     OLLAMA_NOPRUNE             Do not prune model blobs on startup  <br>     OLLAMA_ORIGINS             A comma separated list of allowed origins  <br>     OLLAMA_SCHED_SPREAD        Always schedule model across all GPUs  <br>                                   <br>     OLLAMA_FLASH_ATTENTION     Enabled flash attention  <br>     OLLAMA_KV_CACHE_TYPE       Quantization <span class="hljs-built_in">type</span> <span class="hljs-keyword">for</span> the K/V cache (default: f16)  <br>     OLLAMA_LLM_LIBRARY         Set LLM library to bypass autodetection  <br>     OLLAMA_GPU_OVERHEAD        Reserve a portion of VRAM per GPU (bytes)  <br>     OLLAMA_LOAD_TIMEOUT        How long to allow model loads to stall before giving up (default <span class="hljs-string">&quot;5m&quot;</span>)<br></code></pre></td></tr></table></figure>


<h3 id="切换-Ollama-的模型文件位置"><a href="#切换-Ollama-的模型文件位置" class="headerlink" title="切换 Ollama 的模型文件位置"></a>切换 Ollama 的模型文件位置</h3><ul>
<li>OLLAMA_MODELS 环境变量可以修改模型文件下载的位置。</li>
</ul>
<p>先把选择下载的模型文件移动到新创建的 <code>/mnt/disk</code> , 默认存在的位置是  <code>~/.ollama/models</code><br>（使用 Ollama 官方的脚本安装，默认路径是 <code>/usr/share/ollama/.ollama/</code>）</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-built_in">mv</span> ~/.ollama/ /mnt/disk/<br><span class="hljs-built_in">ln</span> -s /mnt/disk/.ollama ~/.ollama<br></code></pre></td></tr></table></figure>

<p>重新 Ollama 启动测试</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">NVIDIA_VISIBLE_DEVICES=all CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 OLLAMA_ORIGINS=* OLLAMA_NUM_PARALLEL=16 OLLAMA_KEEP_ALIVE=10m ollama serve<br></code></pre></td></tr></table></figure>

<p>测试成功。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">root@pm-65c50001:~# ollama run llama3:8b-instruct-fp16  <br>&gt;&gt;&gt; hi  <br>Hi! It&#x27;s nice to meet you. Is there something I can help you with or would you like to chat?<br></code></pre></td></tr></table></figure>

<h3 id="研究-Ollama-mutlple-GPU"><a href="#研究-Ollama-mutlple-GPU" class="headerlink" title="研究 Ollama mutlple GPU"></a>研究 Ollama mutlple GPU</h3><p>在 <a target="_blank" rel="noopener" href="https://github.com/ollama/ollama/issues/4198">https://github.com/ollama/ollama/issues/4198</a> 中发现一些比较重要的环境变量。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">[Unit]  <br>Description=Ollama Service  <br>After=network-online.target<br><br>[Service]  <br>Environment=&quot;OLLAMA_HOST=0.0.0.0:11434&quot;  <br>Environment=&quot;OLLAMA_ORIGINS=&#x27;*&#x27;&quot;  <br>Environment=&quot;OLLAMA_MODELS=/ollama/ollama/models&quot;  <br>Environment=&quot;OLLAMA_KEEP_ALIVE=10m&quot;  <br>Environment=&quot;OLLAMA_NUM_PARALLEL=4&quot;<br>Environment=&quot;OLLAMA_MAX_LOADED_MODELS=2&quot;  <br>Environment=&quot;CUDA_VISIBLE_DEVICES=0,1,2,3&quot;  <br>ExecStart=/usr/local/bin/ollama serve  <br>User=ollama  <br>Group=ollama  <br>Restart=always  <br>RestartSec=3  <br>Environment=&quot;PATH=/root/.local/bin:/root/bin:/usr/lib64/ccache:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin&quot;<br><br>[Install]  <br>WantedBy=default.target<br></code></pre></td></tr></table></figure>

<p>综合一下，决定使用下面的命令行：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">NVIDIA_VISIBLE_DEVICES=all CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 OLLAMA_ORIGINS=* OLLAMA_NUM_PARALLEL=16 OLLAMA_KEEP_ALIVE=10m ollama serve<br></code></pre></td></tr></table></figure>

<h3 id="如何判断模型是否加载入-GPU"><a href="#如何判断模型是否加载入-GPU" class="headerlink" title="如何判断模型是否加载入 GPU"></a>如何判断模型是否加载入 GPU</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">ollama ps<br>NAME      	ID          	SIZE 	PROCESSOR	UNTIL<br>llama3:70b	bcfb190ca3a7	42 GB	100% GPU 	4 minutes from now<br></code></pre></td></tr></table></figure>

<ul>
<li>48%&#x2F;52% CPU&#x2F;GPU 类似这样的显示，说明模型只有部分加载入 GPU，还有一部分加载入系统的内存</li>
</ul>
<h3 id="内核设置-numa-balancing"><a href="#内核设置-numa-balancing" class="headerlink" title="内核设置 numa_balancing"></a>内核设置 numa_balancing</h3><p>Ollama 0.3.6 加载  Llama 3.1 405b 失败，界面一直卡着不动，在 Github 上看到类似问题。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/ollama/ollama/issues/6425">https://github.com/ollama/ollama/issues/6425</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">time=2024-08-29T10:50:24.720+08:00 level=INFO source=sched.go:445 msg=&quot;loaded runners&quot; count=1<br>time=2024-08-29T10:50:24.720+08:00 level=INFO source=server.go:593 msg=&quot;waiting for llama runner to start responding&quot;<br>time=2024-08-29T10:50:24.720+08:00 level=INFO source=server.go:627 msg=&quot;waiting for server to become available&quot; status=&quot;llm server error&quot;<br>WARNING: /proc/sys/kernel/numa_balancing is enabled, this has been observed to impair performance<br></code></pre></td></tr></table></figure>

<p>上面的 Issue 提到   有影响，手工禁用并使用 Ollama 0.3.8 可以解决问题。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-built_in">echo</span> 0 &gt; /proc/sys/kernel/numa_balancing  <span class="hljs-comment"># or</span><br>sysctl -w kernel.numa_balancing=0<br></code></pre></td></tr></table></figure>

<p>学习 <code>/numa_balancing</code> 的作用，看上去像优化内存使用。<br><a target="_blank" rel="noopener" href="https://docs.kernel.org/admin-guide/sysctl/kernel.html#numa-balancing">https://docs.kernel.org/admin-guide/sysctl/kernel.html#numa-balancing</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">Enables/disables and configures automatic page fault based NUMA memory balancing<br></code></pre></td></tr></table></figure>

<p>Ollama 从 0.37 开始不再是一个独立的 X86_64 的 elf，而是一个 tar.gz<br><a target="_blank" rel="noopener" href="https://github.com/ollama/ollama/releases/download/v0.3.8/ollama-linux-amd64.tgz">https://github.com/ollama/ollama/releases/download/v0.3.8/ollama-linux-amd64.tgz</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs plaintext"># cd root<br># tar zxvf ollama-linux-amd64.tgz    <br>./  <br>./lib/  <br>./lib/ollama/  <br>./lib/ollama/libcublas.so.12.4.2.65  <br>./lib/ollama/libcublasLt.so.11  <br>./lib/ollama/libcublas.so.11.5.1.109  <br>./lib/ollama/libcudart.so.11.3.109  <br>./lib/ollama/libcublas.so.12  <br>./lib/ollama/libcublasLt.so  <br>./lib/ollama/libcublas.so.11  <br>./lib/ollama/libcublas.so  <br>./lib/ollama/libcudart.so  <br>./lib/ollama/libcublasLt.so.12  <br>./lib/ollama/libcublasLt.so.11.5.1.109  <br>./lib/ollama/libcudart.so.11.0  <br>./lib/ollama/libcudart.so.12.4.99  <br>./lib/ollama/libcudart.so.12  <br>./lib/ollama/libcublasLt.so.12.4.2.65  <br>./bin/  <br>./bin/ollama<br></code></pre></td></tr></table></figure>


<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-comment">#export LD_LIBRARY_PATH=/root/lib/ollama/</span><br>OLLAMA_DEBUG=1 NVIDIA_VISIBLE_DEVICES=all CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 OLLAMA_ORIGINS=* OLLAMA_KEEP_ALIVE=10m /root/bin/ollama serve<br></code></pre></td></tr></table></figure>

<p>34s 后，Llama3.1 405b 成功启动</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plaintext">time=2024-08-29T15:07:02.113+08:00 level=INFO source=server.go:630 msg=&quot;llama runner started in 34.94 seconds&quot;<br></code></pre></td></tr></table></figure>


<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[0] NvidiaGraphicsDrivers<br><a target="_blank" rel="noopener" href="https://wiki.debian.org/NvidiaGraphicsDrivers">https://wiki.debian.org/NvidiaGraphicsDrivers</a></p>
<p>[1] Debain backports Instructions<br><a target="_blank" rel="noopener" href="https://backports.debian.org/Instructions/">https://backports.debian.org/Instructions/</a></p>
<p>[2] Ollama on Linux<br><a target="_blank" rel="noopener" href="https://github.com/ollama/ollama/blob/main/docs/linux.md">https://github.com/ollama/ollama/blob/main/docs/linux.md</a></p>
<p>[3] Is it possible &amp; safe to use latest kernel with Debian?<br><a target="_blank" rel="noopener" href="https://unix.stackexchange.com/questions/725783/is-it-possible-safe-to-use-latest-kernel-with-debian">https://unix.stackexchange.com/questions/725783/is-it-possible-safe-to-use-latest-kernel-with-debian</a></p>
<p>[4] Ollama v0.3.0 release note<br><a target="_blank" rel="noopener" href="https://github.com/ollama/ollama/releases/tag/v0.3.0">https://github.com/ollama/ollama/releases/tag/v0.3.0</a></p>
<p>[5] Ollama FAQ<br><a target="_blank" rel="noopener" href="https://github.com/ollama/ollama/blob/main/docs/faq.md#how-do-i-configure-ollama-server">https://github.com/ollama/ollama/blob/main/docs/faq.md#how-do-i-configure-ollama-server</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://usmacd.com">曼福吉</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://usmacd.com/cn/Debian_Nvidia_Ollama/">https://usmacd.com/cn/Debian_Nvidia_Ollama/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://usmacd.com" target="_blank">安全代码</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/linux/">linux</a><a class="post-meta__tags" href="/tags/ai/">ai</a></div><div class="post-share"><div class="social-share" data-image="https://images.unsplash.com/photo-1646026371686-79950ceb6daa?w=640" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/cn/android_apk_extract/" title="如何提取 Android 手机中已经安装的 APK 文件"><img class="cover" src="https://images.unsplash.com/photo-1646026371686-79950ceb6daa?w=640" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">如何提取 Android 手机中已经安装的 APK 文件</div></div><div class="info-2"><div class="info-item-1">首先需要下载 Android Platform Tools 获取 adb 工具。 Android Platform Tools 的下载地址为：https://developer.android.com/tools/releases/platform-tools#downloads 在本机执行下面命令，可以提取 Android 手机上安装的 apk 文件。 123adb shell pm list packagesadb shell pm path &lt;package name&gt;adb pull &lt;apk_path_on_device&gt;  如果要在另外一台手机上安装 Android 应用，则需要在本机执行另外两条命令。 12adb install base.apk # or adb install-multiple base.apk split_config.arm64_v8a.apk *.apk  注：本篇需要一些专业知识，至少需要知道 Android 应用 package name 的含义。 </div></div></div></a><a class="pagination-related" href="/cn/elon_philosophy/" title="马斯克的哲学"><img class="cover" src="https://images.unsplash.com/photo-1646026371686-79950ceb6daa?w=640" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">马斯克的哲学</div></div><div class="info-2"><div class="info-item-1">马斯克的五步工作法马斯克提炼出了一个五步工作法，称之为“算法”，自称每天的工作就是「算法复读机」😄️ 这五个步骤按顺序分别是: 1）质疑每项需求，让要求不那么愚蠢  提出任何一项要求时，都应该附上提出这一要求的人。 永远不要接受一项来自某个部门的要求，比如来自“法务部门”的要求。  2）删除要求当中所有你能删除的部分和流程  虽然你可能还得把它们加回来，如果最后加来的部分还不到删除部分的10%，那就说明删减得还不够。  3）简化和优化  这应该放在第2步之后，因为人们常犯的错误就是简化和优化一个原本不应该存在的部分或者流程。  4）加快周转时间  每个流程都可以加快，但只有遵循了前三个步骤之后才能这么做。  5）自动化  过早的自动化会产生问题，自动化是最后一个步骤，在此之前必须经过质疑、删除、简化等步骤  马斯克五步工作法的重要推论 所有技术经理都必须有实战经验，软件团队的管理人员至少花 20% 的时间编程 犯错没关系，但错了还不肯低头就不行 唯一要遵守的规则就是物理学定律能推导出来的规则，其他一切都只是建议 深度调研需要跨级沟通，直接和你下属的下属交流，不要只和你直接管理的...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/cn/thinking-logic-generative-ai-era/" title="思考的新征程：生成式AI时代的逻辑与思辨"><img class="cover" src="https://images.unsplash.com/photo-1646026371686-79950ceb6daa?w=640" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-12</div><div class="info-item-2">思考的新征程：生成式AI时代的逻辑与思辨</div></div><div class="info-2"><div class="info-item-1">在信息泛滥的数字时代，生成式AI的崛起既带来知识获取便利，也改变了人类与信息的关系。 当ChatGPT能在几秒钟内生成看似专业的论文，我们要问：在这个新世界中，什么将成为人类不可替代的能力？答案是我们古老的认知工具——逻辑思维和思辨能力。 曾经，信息本身是稀缺资源。而今天，信息触手可及，AI将这种获取推向新高度。 当所有人都轻松获取相同信息时，真正的价值不在信息本身， 而是转移到了对信息的判断和利用能力上。 1. 识别AI的海市蜃楼生成式AI有一个特性——“幻觉”。它们可以捏造看似合理但错误的信息。 想象一个情境，某大学教授要求学生使用AI撰写关于特定历史事件的论文，然后用批判性思维分析准确性，许多学生无法识别AI生成内容中的事实错误，甚至有学生为这些错误辩护，因为它们来自“权威”的AI。 在信息过载的世界中，辨别真伪的能力比获取信息更为关键。在AI时代，知识不再是力量，判断力才是。 2. 问题定义：人类思维的堡垒生成式AI擅长回答问题，但在提出有价值的问题方面却不足。 诺贝尔物理学奖获得者理查德·费曼强调，科学是关于理解事物，而不仅仅是知道它们的名称。提出正确的问题比找到答案更...</div></div></div></a><a class="pagination-related" href="/cn/ssd/" title="SSD 迁移记"><img class="cover" src="https://images.unsplash.com/photo-1646026371686-79950ceb6daa?w=640" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2022-06-20</div><div class="info-item-2">SSD 迁移记</div></div><div class="info-2"><div class="info-item-1">由于某些需求，决定上SSD，提高一下硬盘读写速度。上二手东买了三星(SAMSUNG) 860 EVO 最初的想法是作为数据盘使用，即操作系统还是跑在机械硬盘上，仔细一思考，还是折腾一下，要不实在是有些浪费，事实证明，折腾是值得的，感觉就像飞一样。 首先查看一下磁盘原始的情况： 1234$ mount/dev/sda1 on /boot type ext4 (rw,relatime,seclabel,stripe=4)/dev/mapper/fedora-root on / type ext4 (rw,relatime,seclabel)  当然首先要把 SSD 处理一下，安装一下 gparted 图形化界面很好用。 1sudo dnf install gparted  建个分区表，选择 gpt，分个区，/dev/mapper/fedora-root 大小为50G，先分个50G的分区，剩下的全部给 另外一个分区，格式化为 ext4。操作完成后，用fdisk 查看一下： 123456789101112$ fdisk -lDisk /dev/sdb：232.9 GiB，25005935...</div></div></div></a><a class="pagination-related" href="/cn/vim_tips/" title="vim 使用的几个小技巧"><img class="cover" src="https://images.unsplash.com/photo-1646026371686-79950ceb6daa?w=640" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2014-04-10</div><div class="info-item-2">vim 使用的几个小技巧</div></div><div class="info-2"><div class="info-item-1">0.  将tab 替换为空格1:ret  1. 全文格式化normal模式下，gg=G 2. 跳转到上次编辑处normal模式下，&#39;. 3. 替换行尾多余的空格1:%s= *$==  4. Source your vimrc.1:so $MYVIMRC  5. 在文件中跳转normal 模式下，ctrl-o 6. 内置文件浏览器1:Exp 7. 一眼看出tab和空格的区别在.vimrc 中添加下面行， 1set list listchars=tab:&gt;-,trail:.,extends:&gt;   8. 解决 vim 编译时权限不够问题1:w !sudo tee % </div></div></div></a><a class="pagination-related" href="/cn/fedora_linux_7z/" title="Fedora Linux 升级系统中的 7-Zip"><img class="cover" src="https://images.unsplash.com/photo-1646026371686-79950ceb6daa?w=640" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-13</div><div class="info-item-2">Fedora Linux 升级系统中的 7-Zip</div></div><div class="info-2"><div class="info-item-1">早上四哥 (scz) 发了条微博，7-Zip 最近有相关的CVE，CVE-2025-11001、CVE-2025-11002，建议升级到 25.01。 如果使用 Windows 系统，在 https://7-zip.org/ 下载 7-Zip 25.01，升级安装即可。 在 Linux 系统中 7-zip 的版本比较复杂，关于 p7zip 和 7-zip 可以参考7-zip 官方readme.txt 的说明。 123456789101112131415161718192021222324252627282930313233343536377-Zip and p7zip===============Now there are two different ports of 7-Zip for Linux/macOS:1) p7zip - another port of 7-Zip for Linux, made by an independent developer.   The latest version of p7zip now is 16.02, and that p7zip...</div></div></div></a><a class="pagination-related" href="/cn/chrome_flash_update/" title="Google Chrome 浏览器 Adobe Flash Player 升级"><img class="cover" src="https://images.unsplash.com/photo-1646026371686-79950ceb6daa?w=640" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2017-07-14</div><div class="info-item-2">Google Chrome 浏览器 Adobe Flash Player 升级</div></div><div class="info-2"><div class="info-item-1">某一天突然发现网页中flash已经不能正常显示了（我一般都禁用），显示 out of data 错误。 访问 chrome:&#x2F;&#x2F;components&#x2F; 可以升级 Adobe Flash Player Adobe Flash Player - 检查更新， 失败。这里有一个坑，插件中的代理设置是无法影响chrome 内部程序的，必须设置环境变量或者直接使用命令行来设置全局代理。 google-chrome —proxy-server&#x3D;”socks:&#x2F;&#x2F;127.0.0.1:9999” 重新检查更新，可以成功更新了，重启后生效了，已经不报 out of data 错误了。为了保险把系统中flash player也给升级，打开网页 https://get.adobe.com/flashplayer/otherversions/ step 1 选Linux (64 bit), step 2 选 yum，下载后安装，后续可以使用dnf 升级了。 </div></div></div></a><a class="pagination-related" href="/cn/BaiduNetdisk/" title="Linux 下使用百度网盘"><img class="cover" src="https://images.unsplash.com/photo-1646026371686-79950ceb6daa?w=640" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2023-09-06</div><div class="info-item-2">Linux 下使用百度网盘</div></div><div class="info-2"><div class="info-item-1">百度网盘在国内很大概率是绕不过去，稍大点文件都喜欢用百度网盘给下载地址。可是百度网盘下载是限速的，而且对 Linux 用户非常不友好，我的 Fedora 装上 rpm 包也用不了，直接崩溃，所以得想点其他办法。 1. 官方安装包 （不一定都能使用）官方提供了 deb 和 rpm 格式安装包 http://issuecdn.baidupcs.com/issue/netdisk/LinuxGuanjia/3.0.1/baidunetdisk_linux_3.0.1.2.rpmhttp://issuecdn.baidupcs.com/issue/netdisk/LinuxGuanjia/3.0.1/baidunetdisk_linux_3.0.1.2.deb 但是在 Fedora 安装后，会报错误。 2. BaiduPCS-Go （百度网盘客户端 - Go语言编写）https://github.com/iikira/BaiduPCS-Go 2.1 登录.&#x2F;BaiduPCS-Go login -bduss&#x3D;xxxxx 2.2 设置并发1&gt; config set ...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/images/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">曼福吉</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">168</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">17</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="card-info-social-icons"><a class="social-icon" href="/rss.xml" target="_blank" title="RSS"><i class="fas fa-rss" style="color: #4c566a;"></i></a><a class="social-icon" href="https://x.com/henices" target="_blank" title="Twitter"><i class="fab fa-x-twitter" style="color: #4c566a;"></i></a><a class="social-icon" href="mailto:zhouzhenster@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4c566a;"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85-Nvidia-%E9%A9%B1%E5%8A%A8"><span class="toc-number">1.</span> <span class="toc-text">安装 Nvidia 驱动</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85-ollama"><span class="toc-number">2.</span> <span class="toc-text">安装 ollama</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%87%E7%BA%A7-Ollama"><span class="toc-number">3.</span> <span class="toc-text">升级 Ollama</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE-Ollama"><span class="toc-number">4.</span> <span class="toc-text">配置 Ollama</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B8%E8%BD%BD-Ollama"><span class="toc-number">5.</span> <span class="toc-text">卸载 Ollama</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Ollama-%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">6.</span> <span class="toc-text">Ollama 的使用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%87%E6%8D%A2-Ollama-%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%96%87%E4%BB%B6%E4%BD%8D%E7%BD%AE"><span class="toc-number">6.1.</span> <span class="toc-text">切换 Ollama 的模型文件位置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A0%94%E7%A9%B6-Ollama-mutlple-GPU"><span class="toc-number">6.2.</span> <span class="toc-text">研究 Ollama mutlple GPU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%88%A4%E6%96%AD%E6%A8%A1%E5%9E%8B%E6%98%AF%E5%90%A6%E5%8A%A0%E8%BD%BD%E5%85%A5-GPU"><span class="toc-number">6.3.</span> <span class="toc-text">如何判断模型是否加载入 GPU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%85%E6%A0%B8%E8%AE%BE%E7%BD%AE-numa-balancing"><span class="toc-number">6.4.</span> <span class="toc-text">内核设置 numa_balancing</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">7.</span> <span class="toc-text">参考资料</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/cn/investment_category2/" title="金融市场中的不同流派 (2)">金融市场中的不同流派 (2)</a><time datetime="2025-12-09T16:00:00.000Z" title="发表于 2025-12-10 00:00:00">2025-12-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/cn/how_to_set_a_goal/" title="如何设定一个好目标">如何设定一个好目标</a><time datetime="2025-12-04T16:00:00.000Z" title="发表于 2025-12-05 00:00:00">2025-12-05</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/cn/know_Evolution/" title="如何理解演化论的思想">如何理解演化论的思想</a><time datetime="2025-11-20T16:00:00.000Z" title="发表于 2025-11-21 00:00:00">2025-11-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/cn/investment_expectation/" title="金融投资中的预期思维">金融投资中的预期思维</a><time datetime="2025-11-19T16:00:00.000Z" title="发表于 2025-11-20 00:00:00">2025-11-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/cn/duanyongping/" title="段永平退休20多年后罕见公开访谈">段永平退休20多年后罕见公开访谈</a><time datetime="2025-11-11T16:00:00.000Z" title="发表于 2025-11-12 00:00:00">2025-11-12</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(https://images.unsplash.com/photo-1646026371686-79950ceb6daa?w=640);"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 By 曼福吉</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.3"></script><script src="/js/main.js?v=5.5.3"></script><script src="/js/tw_cn.js?v=5.5.3"></script><div class="js-pjax"><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const config = mermaidSrc.dataset.config ? JSON.parse(mermaidSrc.dataset.config) : {}
      if (!config.theme) {
        config.theme = theme
      }
      const mermaidThemeConfig = `%%{init: ${JSON.stringify(config)}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (false) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><i class="fas fa-spinner fa-pulse" id="loading-status" hidden="hidden"></i><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="local-search-input"><input placeholder="搜索文章" type="text"/></div><hr/><div id="local-search-results"></div><div class="ais-Pagination" id="local-search-pagination" style="display:none;"><ul class="ais-Pagination-list"></ul></div><div id="local-search-stats"></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=5.5.3"></script></div></div></body></html>