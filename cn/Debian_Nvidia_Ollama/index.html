<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Nvidia 驱动安装和 Ollama 的使用 | 安全代码</title><meta name="author" content="曼福吉"><meta name="copyright" content="曼福吉"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="根据同事反馈，高版本的 NVIDIA 驱动兼容性有问题，需要安装 Nvidia 驱动 525.147.05 ，过程中可能需要升级内核。 安装 Nvidia 驱动查看 Debian 上显卡安装情况。 12lspci -nn | egrep -i &quot;3d|display|vga&quot;  01:00.0 VGA compatible controller [0300]: NVIDIA C">
<meta property="og:type" content="article">
<meta property="og:title" content="Nvidia 驱动安装和 Ollama 的使用">
<meta property="og:url" content="https://usmacd.com/cn/Debian_Nvidia_Ollama/index.html">
<meta property="og:site_name" content="安全代码">
<meta property="og:description" content="根据同事反馈，高版本的 NVIDIA 驱动兼容性有问题，需要安装 Nvidia 驱动 525.147.05 ，过程中可能需要升级内核。 安装 Nvidia 驱动查看 Debian 上显卡安装情况。 12lspci -nn | egrep -i &quot;3d|display|vga&quot;  01:00.0 VGA compatible controller [0300]: NVIDIA C">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://images.unsplash.com/photo-1646026371686-79950ceb6daa?w=640">
<meta property="article:published_time" content="2024-07-25T16:00:00.000Z">
<meta property="article:modified_time" content="2025-01-01T16:00:00.000Z">
<meta property="article:author" content="曼福吉">
<meta property="article:tag" content="ai">
<meta property="article:tag" content="linux">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://images.unsplash.com/photo-1646026371686-79950ceb6daa?w=640"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Nvidia 驱动安装和 Ollama 的使用",
  "url": "https://usmacd.com/cn/Debian_Nvidia_Ollama/",
  "image": "https://images.unsplash.com/photo-1646026371686-79950ceb6daa?w=640",
  "datePublished": "2024-07-25T16:00:00.000Z",
  "dateModified": "2025-01-01T16:00:00.000Z",
  "author": [
    {
      "@type": "Person",
      "name": "曼福吉",
      "url": "https://usmacd.com"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://usmacd.com/cn/Debian_Nvidia_Ollama/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css?v=5.5.2"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'undefined')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'undefined')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"pagination":{"enable":false,"hitsPerPage":8},"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Nvidia 驱动安装和 Ollama 的使用',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 8.1.0"><link rel="alternate" href="/atom.xml" title="安全代码" type="application/atom+xml">
<link rel="alternate" href="/rss.xml" title="安全代码" type="application/rss+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/images/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">167</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">17</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/thoughts"><i class="fa-fw fas fa-cloud"></i><span> 想法</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/sitemap.xml"><span> sitemap</span></a></div><div class="menus_item"><a class="site-page" href="/random.html"><span> random</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://images.unsplash.com/photo-1646026371686-79950ceb6daa?w=640);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">安全代码</span></a><a class="nav-page-title" href="/"><span class="site-name">Nvidia 驱动安装和 Ollama 的使用</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/thoughts"><i class="fa-fw fas fa-cloud"></i><span> 想法</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/sitemap.xml"><span> sitemap</span></a></div><div class="menus_item"><a class="site-page" href="/random.html"><span> random</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Nvidia 驱动安装和 Ollama 的使用</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-07-25T16:00:00.000Z" title="发表于 2024-07-26 00:00:00">2024-07-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-01-01T16:00:00.000Z" title="更新于 2025-01-02 00:00:00">2025-01-02</time></span></div><div class="meta-secondline"></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p>根据同事反馈，高版本的 NVIDIA 驱动兼容性有问题，需要安装 Nvidia 驱动 525.147.05 ，过程中可能需要升级内核。</p>
<h2 id="安装-Nvidia-驱动"><a href="#安装-Nvidia-驱动" class="headerlink" title="安装 Nvidia 驱动"></a>安装 Nvidia 驱动</h2><p>查看 Debian 上显卡安装情况。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lspci -nn | egrep -i &quot;3d|display|vga&quot;  </span><br><span class="line">01:00.0 VGA compatible controller [0300]: NVIDIA Corporation AD102 [GeForce RTX 4090] [10de:2684] (rev a1)</span><br></pre></td></tr></table></figure>

<p>查看驱动安装具体的情况。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">lsmod | grep nouveau  </span><br><span class="line">nouveau              2433024  0  </span><br><span class="line">mxm_wmi                16384  1 nouveau  </span><br><span class="line">i2c_algo_bit           16384  1 nouveau  </span><br><span class="line">drm_display_helper    184320  1 nouveau  </span><br><span class="line">drm_ttm_helper         16384  1 nouveau  </span><br><span class="line">ttm                    94208  2 drm_ttm_helper,nouveau  </span><br><span class="line">drm_kms_helper        204800  2 drm_display_helper,nouveau  </span><br><span class="line">drm                   614400  5 drm_kms_helper,drm_display_helper,drm_ttm_helper,ttm,nouveau  </span><br><span class="line">video                  65536  2 asus_wmi,nouveau  </span><br><span class="line">wmi                    36864  5 video,asus_wmi,wmi_bmof,mxm_wmi,nouveau  </span><br><span class="line">button                 24576  1 nouveau</span><br></pre></td></tr></table></figure>

<p>看来安装的是开源版本的驱动 <code>nouveau</code>，需要先禁用。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">&quot;blacklist nouveau&quot;</span> | <span class="built_in">sudo</span> <span class="built_in">tee</span> /etc/modprobe.d/nouveau-blacklist.conf</span><br><span class="line"><span class="built_in">sudo</span> update-initramfs -u</span><br><span class="line"><span class="built_in">sudo</span> update-grub</span><br><span class="line"><span class="built_in">sudo</span> reboot</span><br></pre></td></tr></table></figure>

<p>重启后，执行 <code>lsmod | grep nouveau</code>  发现已经返回为空了，成功禁用。</p>
<p>执行命令<code> sudo apt install nvidia-driver firmware-misc-nonfree</code> 安装 NVIDIA Proprietary Driver 报错。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">Consult /var/lib/dkms/nvidia-current/525.147.05/build/make.log for more information.  </span><br><span class="line">dpkg: error processing package nvidia-kernel-dkms (--configure):  </span><br><span class="line">installed nvidia-kernel-dkms package post-installation script subprocess returned error exit status 10  </span><br><span class="line">dpkg: dependency problems prevent configuration of nvidia-driver:  </span><br><span class="line">nvidia-driver depends on nvidia-kernel-dkms (= 525.147.05-4~deb12u1) | nvidia-kernel-525.147.05 | nvidia-open-kernel-525.147.05 | nvidia-open-kernel-525.147.05; however:  </span><br><span class="line"> Package nvidia-kernel-dkms is not configured yet.  </span><br><span class="line"> Package nvidia-kernel-525.147.05 is not installed.  </span><br><span class="line"> Package nvidia-kernel-dkms which provides nvidia-kernel-525.147.05 is not configured yet.  </span><br><span class="line"> Package nvidia-open-kernel-525.147.05 is not installed.  </span><br><span class="line"> Package nvidia-open-kernel-525.147.05 is not installed.  </span><br><span class="line">  </span><br><span class="line">dpkg: error processing package nvidia-driver (--configure):  </span><br><span class="line">dependency problems - leaving unconfigured  </span><br><span class="line">Processing triggers for libc-bin (2.36-9+deb12u4) ...  </span><br><span class="line">Processing triggers for initramfs-tools (0.142) ...  </span><br><span class="line">update-initramfs: Generating /boot/initrd.img-6.1.0-18-amd64  </span><br><span class="line">Processing triggers for update-glx (1.2.2) ...  </span><br><span class="line">Processing triggers for glx-alternative-nvidia (1.2.2) ...  </span><br><span class="line">update-alternatives: using /usr/lib/nvidia to provide /usr/lib/glx (glx) in auto mode  </span><br><span class="line">Processing triggers for glx-alternative-mesa (1.2.2) ...  </span><br><span class="line">Processing triggers for libc-bin (2.36-9+deb12u4) ...  </span><br><span class="line">Processing triggers for initramfs-tools (0.142) ...  </span><br><span class="line">update-initramfs: Generating /boot/initrd.img-6.1.0-18-amd64  </span><br><span class="line">Errors were encountered while processing:  </span><br><span class="line">nvidia-kernel-dkms  </span><br><span class="line">nvidia-driver  </span><br><span class="line">E: Sub-process /usr/bin/dpkg returned an error code (1)</span><br></pre></td></tr></table></figure>

<p>确认 debian 版本 <code>lsb_release -a</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">No LSB modules are available.  </span><br><span class="line">Distributor ID: Debian  </span><br><span class="line">Description:    Debian GNU/Linux 12 (bookworm)  </span><br><span class="line">Release:        12  </span><br><span class="line">Codename:       bookworm</span><br></pre></td></tr></table></figure>

<p>根据 stackexchange 上的回答 ，安全升级 Debian 内核的方法是使用 backports 安装。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">&quot;deb http://deb.debian.org/debian bookworm-backports main&quot;</span> | <span class="built_in">sudo</span> <span class="built_in">tee</span> /etc/apt/sources.list.d/debian-backports.list</span><br><span class="line"><span class="built_in">sudo</span> apt update</span><br><span class="line"><span class="built_in">sudo</span> apt install -t bookworm-backports linux-image-amd64</span><br><span class="line"><span class="built_in">sudo</span> reboot</span><br></pre></td></tr></table></figure>

<p>重新启动后，执行 <code>uname -a</code>  发现内核已经成功升级了。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">uname -a                                                                          </span><br><span class="line">Linux debian 6.7.12+bpo-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.7.12-1~bpo12+1 (2024-05-06) x86_64 GNU/Linux</span><br></pre></td></tr></table></figure>

<p>重新安装 NVIDIA Proprietary Driver <code>sudo apt install nvidia-driver firmware-misc-nonfree</code> ，这次没有报错了。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi    </span><br><span class="line"></span><br><span class="line">NVIDIA-SMI 525.147.05   Driver Version: 525.147.05   CUDA Version: 12.0</span><br></pre></td></tr></table></figure>

<p>NVIDIA Proprietary Driver 525 感觉有问题，过了一段时间后机器出现重启现象，dmesg 显示错误 <code>ACPI BIOS Error (bug)</code> 。 </p>
<p>上网搜索错误，有人反馈是 525 驱动问题（不确定）。Debain 系统 Nvidia 驱动有更新，执行 <code>apt upgrade</code> 后成功升级到 535 。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># nvidia-smi</span><br><span class="line"></span><br><span class="line">NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2</span><br></pre></td></tr></table></figure>

<p>升级 Nvidia 驱动到 535 后，暂未出现重启现象。</p>
<h2 id="安装-ollama"><a href="#安装-ollama" class="headerlink" title="安装 ollama"></a>安装 ollama</h2><p>执行下面的命令安装 Ollama</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -fsSL https://ollama.com/install.sh | sh</span><br></pre></td></tr></table></figure>

<p>下载速度很慢，还是挂线路。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export https_proxy=http://127.0.0.1:7890</span><br><span class="line">export http_proxy=http://127.0.0.1:7890</span><br><span class="line">curl -fsSL https://ollama.com/install.sh | sh</span><br></pre></td></tr></table></figure>

<p>挂上线路后，很快 Ollama 就安装成功了。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; Downloading ollama...  </span><br><span class="line">######################################################################## 100.0%#=#=-#  #                                                                        </span><br><span class="line">&gt;&gt;&gt; Installing ollama to /usr/local/bin...  </span><br><span class="line">&gt;&gt;&gt; Creating ollama user...  </span><br><span class="line">&gt;&gt;&gt; Adding ollama user to render group...  </span><br><span class="line">&gt;&gt;&gt; Adding ollama user to video group...  </span><br><span class="line">&gt;&gt;&gt; Adding current user to ollama group...  </span><br><span class="line">&gt;&gt;&gt; Creating ollama systemd service...  </span><br><span class="line">&gt;&gt;&gt; Enabling and starting ollama service...  </span><br><span class="line">Created symlink /etc/systemd/system/default.target.wants/ollama.service → /etc/systemd/system/ollama.service.  </span><br><span class="line">&gt;&gt;&gt; NVIDIA GPU installed.</span><br></pre></td></tr></table></figure>

<p>ollama 下载 llama3 8b 和 qwen2 7b 模型，执行下面的命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ollama pull llama3</span><br><span class="line">ollama pull qwen2:7b</span><br></pre></td></tr></table></figure>

<p>测试 llama3 模型，运行正常。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ollama run llama3  </span><br><span class="line">&gt;&gt;&gt; hi  </span><br><span class="line">Hi! It&#x27;s nice to meet you. Is there something I can help you with or would you like to chat?</span><br></pre></td></tr></table></figure>
<h2 id="升级-Ollama"><a href="#升级-Ollama" class="headerlink" title="升级 Ollama"></a>升级 Ollama</h2><p>Ollama 0.3.0 支持通过 llama3.1 进行工具调用，有必要升级。参见 [4]</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> curl -L https://ollama.com/download/ollama-linux-amd64 -o /usr/local/bin/ollama</span><br><span class="line"><span class="built_in">sudo</span> <span class="built_in">chmod</span> +x /usr/local/bin/ollama</span><br></pre></td></tr></table></figure>

<p>升级完毕，需要重启 Ollama 服务。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> systemctl daemon-reload</span><br><span class="line"><span class="built_in">sudo</span> systemctl restart ollama</span><br></pre></td></tr></table></figure>

<p>新版本的 Ollama 已经不是一个单独的文件，而是一个 <code>tar.gz</code> 的压缩包。<br>tar.tgz 中包含了 ollama 运行需要的动态库，在升级前需要将这些动态库删除。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> <span class="built_in">rm</span> -rf /usr/lib/ollama</span><br><span class="line"><span class="built_in">sudo</span> <span class="built_in">rm</span> -rf /usr/local/lib/ollama</span><br><span class="line"></span><br><span class="line">curl -L https://ollama.com/download/ollama-linux-amd64.tgz -o ollama-linux-amd64.tgz</span><br><span class="line"><span class="built_in">sudo</span> tar -C /usr/local -xzf ollama-linux-amd64.tgz</span><br></pre></td></tr></table></figure>

<p>执行完上述命令后，启动 ollama 检查版本确认是否升级成功。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ollama serve</span><br><span class="line">ollama -v</span><br></pre></td></tr></table></figure>

<h2 id="配置-Ollama"><a href="#配置-Ollama" class="headerlink" title="配置 Ollama"></a>配置 Ollama</h2><p>如果需要在浏览器插件（比如沉浸翻译）中调用 Ollama api，涉及 Cross-Origin 访问，需要修改 Ollama 配置。</p>
<p>官方文档提到了相关的设置 [5]，用 vim 直接修改 &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;ollama.service 中，添加下面内容：</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Environment</span>=<span class="string">&quot;OLLAMA_HOST=*&quot;</span></span><br></pre></td></tr></table></figure>

<p>重启 Ollama 服务</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> systemctl daemon-reload</span><br><span class="line"><span class="built_in">sudo</span> systemctl restart ollama</span><br></pre></td></tr></table></figure>

<p>在远程主机上，查看 Ollama 端口侦听情况</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">apt install net-tools</span><br><span class="line">netstat -antp | grep -i ollama  </span><br></pre></td></tr></table></figure>

<p>Ollama 默认侦听 127.0.0.1:11434</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tcp        0      0 127.0.0.1:11434         0.0.0.0:*               LISTEN      50508/ollama</span><br></pre></td></tr></table></figure>

<p>利用 SSH 将远程主机 Ollama 侦听的端口 11434 转发到本地 127.0.0.1:11434</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -N -g -L 127.0.0.1:11434:127.0.0.1:11434 root@1.1.1.1  <span class="comment"># 将 1.1.1.1 替换成你的 ip</span></span><br></pre></td></tr></table></figure>
<h2 id="卸载-Ollama"><a href="#卸载-Ollama" class="headerlink" title="卸载 Ollama"></a>卸载 Ollama</h2><p>停止 Ollama 服务</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> systemctl stop ollama</span><br><span class="line"><span class="built_in">sudo</span> systemctl <span class="built_in">disable</span> ollama</span><br><span class="line"><span class="built_in">sudo</span> <span class="built_in">rm</span> /etc/systemd/system/ollama.service</span><br></pre></td></tr></table></figure>

<p>删除二进制文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo rm $(which ollama)</span><br></pre></td></tr></table></figure>

<p>删除 Ollama 用户</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> <span class="built_in">rm</span> -r /usr/share/ollama</span><br><span class="line"><span class="built_in">sudo</span> userdel ollama</span><br><span class="line"><span class="built_in">sudo</span> groupdel ollama</span><br></pre></td></tr></table></figure>

<h2 id="Ollama-的使用"><a href="#Ollama-的使用" class="headerlink" title="Ollama 的使用"></a>Ollama 的使用</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OLLAMA_ORIGINS=* OLLAMA_NUM_PARALLEL=16 ollama serve</span><br></pre></td></tr></table></figure>

<ul>
<li>OLLAMA_ORIGINS 跨域设置</li>
<li>OLLAMA_NUM_PARALLEL 支持的并行请求数量</li>
<li>OLLAMA_DEBUG 打印调试信息</li>
<li>OLLAMA_LLM_LIBRARY 支持下面的选项  rocm_v6 cpu cpu_avx cpu_avx2 cuda_v11 rocm_v5</li>
<li>OLLAMA_KEEP_ALIVE 模型在显存内加载的时间，默认为 5 分钟</li>
<li>OLLAMA_GPU_OVERHEAD 单独为每个 GPU 预留的 VRAM ，单位是字节</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://github.com/ollama/ollama/blob/main/envconfig/config.go">https://github.com/ollama/ollama/blob/main/envconfig/config.go</a></p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">AsMap</span><span class="params">()</span></span> <span class="keyword">map</span>[<span class="type">string</span>]EnvVar &#123;</span><br><span class="line">	ret := <span class="keyword">map</span>[<span class="type">string</span>]EnvVar&#123;</span><br><span class="line">		<span class="string">&quot;OLLAMA_DEBUG&quot;</span>:             &#123;<span class="string">&quot;OLLAMA_DEBUG&quot;</span>, Debug(), <span class="string">&quot;Show additional debug information (e.g. OLLAMA_DEBUG=1)&quot;</span>&#125;,</span><br><span class="line">		<span class="string">&quot;OLLAMA_FLASH_ATTENTION&quot;</span>:   &#123;<span class="string">&quot;OLLAMA_FLASH_ATTENTION&quot;</span>, FlashAttention(), <span class="string">&quot;Enabled flash attention&quot;</span>&#125;,</span><br><span class="line">		<span class="string">&quot;OLLAMA_HOST&quot;</span>:              &#123;<span class="string">&quot;OLLAMA_HOST&quot;</span>, Host(), <span class="string">&quot;IP Address for the ollama server (default 127.0.0.1:11434)&quot;</span>&#125;,</span><br><span class="line">		<span class="string">&quot;OLLAMA_KEEP_ALIVE&quot;</span>:        &#123;<span class="string">&quot;OLLAMA_KEEP_ALIVE&quot;</span>, KeepAlive(), <span class="string">&quot;The duration that models stay loaded in memory (default \&quot;5m\&quot;)&quot;</span>&#125;,</span><br><span class="line">		<span class="string">&quot;OLLAMA_LLM_LIBRARY&quot;</span>:       &#123;<span class="string">&quot;OLLAMA_LLM_LIBRARY&quot;</span>, LLMLibrary(), <span class="string">&quot;Set LLM library to bypass autodetection&quot;</span>&#125;,</span><br><span class="line">		<span class="string">&quot;OLLAMA_MAX_LOADED_MODELS&quot;</span>: &#123;<span class="string">&quot;OLLAMA_MAX_LOADED_MODELS&quot;</span>, MaxRunners(), <span class="string">&quot;Maximum number of loaded models per GPU&quot;</span>&#125;,</span><br><span class="line">		<span class="string">&quot;OLLAMA_MAX_QUEUE&quot;</span>:         &#123;<span class="string">&quot;OLLAMA_MAX_QUEUE&quot;</span>, MaxQueue(), <span class="string">&quot;Maximum number of queued requests&quot;</span>&#125;,</span><br><span class="line">		<span class="string">&quot;OLLAMA_MODELS&quot;</span>:            &#123;<span class="string">&quot;OLLAMA_MODELS&quot;</span>, Models(), <span class="string">&quot;The path to the models directory&quot;</span>&#125;,</span><br><span class="line">		<span class="string">&quot;OLLAMA_NOHISTORY&quot;</span>:         &#123;<span class="string">&quot;OLLAMA_NOHISTORY&quot;</span>, NoHistory(), <span class="string">&quot;Do not preserve readline history&quot;</span>&#125;,</span><br><span class="line">		<span class="string">&quot;OLLAMA_NOPRUNE&quot;</span>:           &#123;<span class="string">&quot;OLLAMA_NOPRUNE&quot;</span>, NoPrune(), <span class="string">&quot;Do not prune model blobs on startup&quot;</span>&#125;,</span><br><span class="line">		<span class="string">&quot;OLLAMA_NUM_PARALLEL&quot;</span>:      &#123;<span class="string">&quot;OLLAMA_NUM_PARALLEL&quot;</span>, NumParallel(), <span class="string">&quot;Maximum number of parallel requests&quot;</span>&#125;,</span><br><span class="line">		<span class="string">&quot;OLLAMA_ORIGINS&quot;</span>:           &#123;<span class="string">&quot;OLLAMA_ORIGINS&quot;</span>, Origins(), <span class="string">&quot;A comma separated list of allowed origins&quot;</span>&#125;,</span><br><span class="line">		<span class="string">&quot;OLLAMA_RUNNERS_DIR&quot;</span>:       &#123;<span class="string">&quot;OLLAMA_RUNNERS_DIR&quot;</span>, RunnersDir(), <span class="string">&quot;Location for runners&quot;</span>&#125;,</span><br><span class="line">		<span class="string">&quot;OLLAMA_SCHED_SPREAD&quot;</span>:      &#123;<span class="string">&quot;OLLAMA_SCHED_SPREAD&quot;</span>, SchedSpread(), <span class="string">&quot;Always schedule model across all GPUs&quot;</span>&#125;,</span><br><span class="line">		<span class="string">&quot;OLLAMA_TMPDIR&quot;</span>:            &#123;<span class="string">&quot;OLLAMA_TMPDIR&quot;</span>, TmpDir(), <span class="string">&quot;Location for temporary files&quot;</span>&#125;,</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">if</span> runtime.GOOS != <span class="string">&quot;darwin&quot;</span> &#123;</span><br><span class="line">		ret[<span class="string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = EnvVar&#123;<span class="string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>, CudaVisibleDevices(), <span class="string">&quot;Set which NVIDIA devices are visible&quot;</span>&#125;</span><br><span class="line">		ret[<span class="string">&quot;HIP_VISIBLE_DEVICES&quot;</span>] = EnvVar&#123;<span class="string">&quot;HIP_VISIBLE_DEVICES&quot;</span>, HipVisibleDevices(), <span class="string">&quot;Set which AMD devices are visible&quot;</span>&#125;</span><br><span class="line">		ret[<span class="string">&quot;ROCR_VISIBLE_DEVICES&quot;</span>] = EnvVar&#123;<span class="string">&quot;ROCR_VISIBLE_DEVICES&quot;</span>, RocrVisibleDevices(), <span class="string">&quot;Set which AMD devices are visible&quot;</span>&#125;</span><br><span class="line">		ret[<span class="string">&quot;GPU_DEVICE_ORDINAL&quot;</span>] = EnvVar&#123;<span class="string">&quot;GPU_DEVICE_ORDINAL&quot;</span>, GpuDeviceOrdinal(), <span class="string">&quot;Set which AMD devices are visible&quot;</span>&#125;</span><br><span class="line">		ret[<span class="string">&quot;HSA_OVERRIDE_GFX_VERSION&quot;</span>] = EnvVar&#123;<span class="string">&quot;HSA_OVERRIDE_GFX_VERSION&quot;</span>, HsaOverrideGfxVersion(), <span class="string">&quot;Override the gfx used for all detected AMD GPUs&quot;</span>&#125;</span><br><span class="line">		ret[<span class="string">&quot;OLLAMA_INTEL_GPU&quot;</span>] = EnvVar&#123;<span class="string">&quot;OLLAMA_INTEL_GPU&quot;</span>, IntelGPU(), <span class="string">&quot;Enable experimental Intel GPU detection&quot;</span>&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> ret</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以使用 <code>ollama serve -h</code> 查看官方明确支持的环境变量。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">ollama serve -h  </span><br><span class="line">Start ollama  </span><br><span class="line">  </span><br><span class="line">Usage:  </span><br><span class="line"> ollama serve [flags]  </span><br><span class="line">  </span><br><span class="line">Aliases:  </span><br><span class="line"> serve, start  </span><br><span class="line">  </span><br><span class="line">Flags:  </span><br><span class="line"> -h, --<span class="built_in">help</span>   <span class="built_in">help</span> <span class="keyword">for</span> serve  </span><br><span class="line">  </span><br><span class="line">Environment Variables:  </span><br><span class="line">     OLLAMA_DEBUG               Show additional debug information (e.g. OLLAMA_DEBUG=1)  </span><br><span class="line">     OLLAMA_HOST                IP Address <span class="keyword">for</span> the ollama server (default 127.0.0.1:11434)  </span><br><span class="line">     OLLAMA_KEEP_ALIVE          The duration that models stay loaded <span class="keyword">in</span> memory (default <span class="string">&quot;5m&quot;</span>)  </span><br><span class="line">     OLLAMA_MAX_LOADED_MODELS   Maximum number of loaded models per GPU  </span><br><span class="line">     OLLAMA_MAX_QUEUE           Maximum number of queued requests  </span><br><span class="line">     OLLAMA_MODELS              The path to the models directory  </span><br><span class="line">     OLLAMA_NUM_PARALLEL        Maximum number of parallel requests  </span><br><span class="line">     OLLAMA_NOPRUNE             Do not prune model blobs on startup  </span><br><span class="line">     OLLAMA_ORIGINS             A comma separated list of allowed origins  </span><br><span class="line">     OLLAMA_SCHED_SPREAD        Always schedule model across all GPUs  </span><br><span class="line">                                   </span><br><span class="line">     OLLAMA_FLASH_ATTENTION     Enabled flash attention  </span><br><span class="line">     OLLAMA_KV_CACHE_TYPE       Quantization <span class="built_in">type</span> <span class="keyword">for</span> the K/V cache (default: f16)  </span><br><span class="line">     OLLAMA_LLM_LIBRARY         Set LLM library to bypass autodetection  </span><br><span class="line">     OLLAMA_GPU_OVERHEAD        Reserve a portion of VRAM per GPU (bytes)  </span><br><span class="line">     OLLAMA_LOAD_TIMEOUT        How long to allow model loads to stall before giving up (default <span class="string">&quot;5m&quot;</span>)</span><br></pre></td></tr></table></figure>


<h3 id="切换-Ollama-的模型文件位置"><a href="#切换-Ollama-的模型文件位置" class="headerlink" title="切换 Ollama 的模型文件位置"></a>切换 Ollama 的模型文件位置</h3><ul>
<li>OLLAMA_MODELS 环境变量可以修改模型文件下载的位置。</li>
</ul>
<p>先把选择下载的模型文件移动到新创建的 <code>/mnt/disk</code> , 默认存在的位置是  <code>~/.ollama/models</code><br>（使用 Ollama 官方的脚本安装，默认路径是 <code>/usr/share/ollama/.ollama/</code>）</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mv</span> ~/.ollama/ /mnt/disk/</span><br><span class="line"><span class="built_in">ln</span> -s /mnt/disk/.ollama ~/.ollama</span><br></pre></td></tr></table></figure>

<p>重新 Ollama 启动测试</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">NVIDIA_VISIBLE_DEVICES=all CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 OLLAMA_ORIGINS=* OLLAMA_NUM_PARALLEL=16 OLLAMA_KEEP_ALIVE=10m ollama serve</span><br></pre></td></tr></table></figure>

<p>测试成功。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@pm-65c50001:~# ollama run llama3:8b-instruct-fp16  </span><br><span class="line">&gt;&gt;&gt; hi  </span><br><span class="line">Hi! It&#x27;s nice to meet you. Is there something I can help you with or would you like to chat?</span><br></pre></td></tr></table></figure>

<h3 id="研究-Ollama-mutlple-GPU"><a href="#研究-Ollama-mutlple-GPU" class="headerlink" title="研究 Ollama mutlple GPU"></a>研究 Ollama mutlple GPU</h3><p>在 <a target="_blank" rel="noopener" href="https://github.com/ollama/ollama/issues/4198">https://github.com/ollama/ollama/issues/4198</a> 中发现一些比较重要的环境变量。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[Unit]  </span><br><span class="line">Description=Ollama Service  </span><br><span class="line">After=network-online.target</span><br><span class="line"></span><br><span class="line">[Service]  </span><br><span class="line">Environment=&quot;OLLAMA_HOST=0.0.0.0:11434&quot;  </span><br><span class="line">Environment=&quot;OLLAMA_ORIGINS=&#x27;*&#x27;&quot;  </span><br><span class="line">Environment=&quot;OLLAMA_MODELS=/ollama/ollama/models&quot;  </span><br><span class="line">Environment=&quot;OLLAMA_KEEP_ALIVE=10m&quot;  </span><br><span class="line">Environment=&quot;OLLAMA_NUM_PARALLEL=4&quot;</span><br><span class="line">Environment=&quot;OLLAMA_MAX_LOADED_MODELS=2&quot;  </span><br><span class="line">Environment=&quot;CUDA_VISIBLE_DEVICES=0,1,2,3&quot;  </span><br><span class="line">ExecStart=/usr/local/bin/ollama serve  </span><br><span class="line">User=ollama  </span><br><span class="line">Group=ollama  </span><br><span class="line">Restart=always  </span><br><span class="line">RestartSec=3  </span><br><span class="line">Environment=&quot;PATH=/root/.local/bin:/root/bin:/usr/lib64/ccache:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin&quot;</span><br><span class="line"></span><br><span class="line">[Install]  </span><br><span class="line">WantedBy=default.target</span><br></pre></td></tr></table></figure>

<p>综合一下，决定使用下面的命令行：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">NVIDIA_VISIBLE_DEVICES=all CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 OLLAMA_ORIGINS=* OLLAMA_NUM_PARALLEL=16 OLLAMA_KEEP_ALIVE=10m ollama serve</span><br></pre></td></tr></table></figure>

<h3 id="如何判断模型是否加载入-GPU"><a href="#如何判断模型是否加载入-GPU" class="headerlink" title="如何判断模型是否加载入 GPU"></a>如何判断模型是否加载入 GPU</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ollama ps</span><br><span class="line">NAME      	ID          	SIZE 	PROCESSOR	UNTIL</span><br><span class="line">llama3:70b	bcfb190ca3a7	42 GB	100% GPU 	4 minutes from now</span><br></pre></td></tr></table></figure>

<ul>
<li>48%&#x2F;52% CPU&#x2F;GPU 类似这样的显示，说明模型只有部分加载入 GPU，还有一部分加载入系统的内存</li>
</ul>
<h3 id="内核设置-numa-balancing"><a href="#内核设置-numa-balancing" class="headerlink" title="内核设置 numa_balancing"></a>内核设置 numa_balancing</h3><p>Ollama 0.3.6 加载  Llama 3.1 405b 失败，界面一直卡着不动，在 Github 上看到类似问题。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/ollama/ollama/issues/6425">https://github.com/ollama/ollama/issues/6425</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">time=2024-08-29T10:50:24.720+08:00 level=INFO source=sched.go:445 msg=&quot;loaded runners&quot; count=1</span><br><span class="line">time=2024-08-29T10:50:24.720+08:00 level=INFO source=server.go:593 msg=&quot;waiting for llama runner to start responding&quot;</span><br><span class="line">time=2024-08-29T10:50:24.720+08:00 level=INFO source=server.go:627 msg=&quot;waiting for server to become available&quot; status=&quot;llm server error&quot;</span><br><span class="line">WARNING: /proc/sys/kernel/numa_balancing is enabled, this has been observed to impair performance</span><br></pre></td></tr></table></figure>

<p>上面的 Issue 提到   有影响，手工禁用并使用 Ollama 0.3.8 可以解决问题。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> 0 &gt; /proc/sys/kernel/numa_balancing  <span class="comment"># or</span></span><br><span class="line">sysctl -w kernel.numa_balancing=0</span><br></pre></td></tr></table></figure>

<p>学习 <code>/numa_balancing</code> 的作用，看上去像优化内存使用。<br><a target="_blank" rel="noopener" href="https://docs.kernel.org/admin-guide/sysctl/kernel.html#numa-balancing">https://docs.kernel.org/admin-guide/sysctl/kernel.html#numa-balancing</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Enables/disables and configures automatic page fault based NUMA memory balancing</span><br></pre></td></tr></table></figure>

<p>Ollama 从 0.37 开始不再是一个独立的 X86_64 的 elf，而是一个 tar.gz<br><a target="_blank" rel="noopener" href="https://github.com/ollama/ollama/releases/download/v0.3.8/ollama-linux-amd64.tgz">https://github.com/ollama/ollama/releases/download/v0.3.8/ollama-linux-amd64.tgz</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># cd root</span><br><span class="line"># tar zxvf ollama-linux-amd64.tgz    </span><br><span class="line">./  </span><br><span class="line">./lib/  </span><br><span class="line">./lib/ollama/  </span><br><span class="line">./lib/ollama/libcublas.so.12.4.2.65  </span><br><span class="line">./lib/ollama/libcublasLt.so.11  </span><br><span class="line">./lib/ollama/libcublas.so.11.5.1.109  </span><br><span class="line">./lib/ollama/libcudart.so.11.3.109  </span><br><span class="line">./lib/ollama/libcublas.so.12  </span><br><span class="line">./lib/ollama/libcublasLt.so  </span><br><span class="line">./lib/ollama/libcublas.so.11  </span><br><span class="line">./lib/ollama/libcublas.so  </span><br><span class="line">./lib/ollama/libcudart.so  </span><br><span class="line">./lib/ollama/libcublasLt.so.12  </span><br><span class="line">./lib/ollama/libcublasLt.so.11.5.1.109  </span><br><span class="line">./lib/ollama/libcudart.so.11.0  </span><br><span class="line">./lib/ollama/libcudart.so.12.4.99  </span><br><span class="line">./lib/ollama/libcudart.so.12  </span><br><span class="line">./lib/ollama/libcublasLt.so.12.4.2.65  </span><br><span class="line">./bin/  </span><br><span class="line">./bin/ollama</span><br></pre></td></tr></table></figure>


<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#export LD_LIBRARY_PATH=/root/lib/ollama/</span></span><br><span class="line">OLLAMA_DEBUG=1 NVIDIA_VISIBLE_DEVICES=all CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 OLLAMA_ORIGINS=* OLLAMA_KEEP_ALIVE=10m /root/bin/ollama serve</span><br></pre></td></tr></table></figure>

<p>34s 后，Llama3.1 405b 成功启动</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">time=2024-08-29T15:07:02.113+08:00 level=INFO source=server.go:630 msg=&quot;llama runner started in 34.94 seconds&quot;</span><br></pre></td></tr></table></figure>


<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[0] NvidiaGraphicsDrivers<br><a target="_blank" rel="noopener" href="https://wiki.debian.org/NvidiaGraphicsDrivers">https://wiki.debian.org/NvidiaGraphicsDrivers</a></p>
<p>[1] Debain backports Instructions<br><a target="_blank" rel="noopener" href="https://backports.debian.org/Instructions/">https://backports.debian.org/Instructions/</a></p>
<p>[2] Ollama on Linux<br><a target="_blank" rel="noopener" href="https://github.com/ollama/ollama/blob/main/docs/linux.md">https://github.com/ollama/ollama/blob/main/docs/linux.md</a></p>
<p>[3] Is it possible &amp; safe to use latest kernel with Debian?<br><a target="_blank" rel="noopener" href="https://unix.stackexchange.com/questions/725783/is-it-possible-safe-to-use-latest-kernel-with-debian">https://unix.stackexchange.com/questions/725783/is-it-possible-safe-to-use-latest-kernel-with-debian</a></p>
<p>[4] Ollama v0.3.0 release note<br><a target="_blank" rel="noopener" href="https://github.com/ollama/ollama/releases/tag/v0.3.0">https://github.com/ollama/ollama/releases/tag/v0.3.0</a></p>
<p>[5] Ollama FAQ<br><a target="_blank" rel="noopener" href="https://github.com/ollama/ollama/blob/main/docs/faq.md#how-do-i-configure-ollama-server">https://github.com/ollama/ollama/blob/main/docs/faq.md#how-do-i-configure-ollama-server</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://usmacd.com">曼福吉</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://usmacd.com/cn/Debian_Nvidia_Ollama/">https://usmacd.com/cn/Debian_Nvidia_Ollama/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://usmacd.com" target="_blank">安全代码</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/linux/">linux</a><a class="post-meta__tags" href="/tags/ai/">ai</a></div><div class="post-share"><div class="social-share" data-image="https://images.unsplash.com/photo-1646026371686-79950ceb6daa?w=640" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/cn/android_apk_extract/" title="如何提取 Android 手机中已经安装的 APK 文件"><img class="cover" src="https://images.unsplash.com/photo-1646026371686-79950ceb6daa?w=640" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">如何提取 Android 手机中已经安装的 APK 文件</div></div><div class="info-2"><div class="info-item-1">首先需要下载 Android Platform Tools 获取 adb 工具。 Android Platform Tools 的下载地址为：https://developer.android.com/tools/releases/platform-tools#downloads 在本机执行下面命令，可以提取 Android 手机上安装的 apk 文件。 123adb shell pm list packagesadb shell pm path &lt;package name&gt;adb pull &lt;apk_path_on_device&gt;  如果要在另外一台手机上安装 Android 应用，则需要在本机执行另外两条命令。 12adb install base.apk # or adb install-multiple base.apk split_config.arm64_v8a.apk *.apk  注：本篇需要一些专业知识，至少需要知道 Android 应用 package name 的含义。 </div></div></div></a><a class="pagination-related" href="/cn/elon_philosophy/" title="马斯克的哲学"><img class="cover" src="https://images.unsplash.com/photo-1646026371686-79950ceb6daa?w=640" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">马斯克的哲学</div></div><div class="info-2"><div class="info-item-1">马斯克的五步工作法马斯克提炼出了一个五步工作法，称之为“算法”，自称每天的工作就是「算法复读机」😄️ 这五个步骤按顺序分别是: 1）质疑每项需求，让要求不那么愚蠢  提出任何一项要求时，都应该附上提出这一要求的人。 永远不要接受一项来自某个部门的要求，比如来自“法务部门”的要求。  2）删除要求当中所有你能删除的部分和流程  虽然你可能还得把它们加回来，如果最后加来的部分还不到删除部分的10%，那就说明删减得还不够。  3）简化和优化  这应该放在第2步之后，因为人们常犯的错误就是简化和优化一个原本不应该存在的部分或者流程。  4）加快周转时间  每个流程都可以加快，但只有遵循了前三个步骤之后才能这么做。  5）自动化  过早的自动化会产生问题，自动化是最后一个步骤，在此之前必须经过质疑、删除、简化等步骤  马斯克五步工作法的重要推论 所有技术经理都必须有实战经验，软件团队的管理人员至少花 20% 的时间编程 犯错没关系，但错了还不肯低头就不行 唯一要遵守的规则就是物理学定律能推导出来的规则，其他一切都只是建议 深度调研需要跨级沟通，直接和你下属的下属交流，不要只和你直接管理的...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/cn/fedora_linux_7z/" title="Fedora Linux 升级系统中的 7-Zip"><img class="cover" src="https://images.unsplash.com/photo-1646026371686-79950ceb6daa?w=640" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-13</div><div class="info-item-2">Fedora Linux 升级系统中的 7-Zip</div></div><div class="info-2"><div class="info-item-1">早上四哥 (scz) 发了条微博，7-Zip 最近有相关的CVE，CVE-2025-11001、CVE-2025-11002，建议升级到 25.01。 如果使用 Windows 系统，在 https://7-zip.org/ 下载 7-Zip 25.01，升级安装即可。 在 Linux 系统中 7-zip 的版本比较复杂，关于 p7zip 和 7-zip 可以参考7-zip 官方readme.txt 的说明。 123456789101112131415161718192021222324252627282930313233343536377-Zip and p7zip===============Now there are two different ports of 7-Zip for Linux/macOS:1) p7zip - another port of 7-Zip for Linux, made by an independent developer.   The latest version of p7zip now is 16.02, and that p7zip...</div></div></div></a><a class="pagination-related" href="/cn/vmware_fix_guest_disk_error/" title="修复 VMware Workstation Ubuntu Guest 磁盘错误"><img class="cover" src="https://images.unsplash.com/photo-1646026371686-79950ceb6daa?w=640" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-23</div><div class="info-item-2">修复 VMware Workstation Ubuntu Guest 磁盘错误</div></div><div class="info-2"><div class="info-item-1">VMware Workstation 17.6.4 build-24832109 不稳定，几次出错后 Ubuntu Guest 无法正常登录系统。 主要异常为：输入用户和密码后卡在 GUI 的登录界面，使用 fsck 修复分区后问题解决，具体步骤如下：  菜单：VMWare -&gt; 虚拟机 -&gt; SSH -&gt; 连接到 SSH，弹出 cmd ，输入用户密码后成功登录。  登录后发现 Ubuntu 系统报错信息。 1234Last login: Tue Sep 16 10:37:29 2025 from 127.0.0.1Could not chdir to home directory /home/zz: No such file or directoryTo run a command as administrator (user &quot;root&quot;), use &quot;sudo &lt;command&gt;&quot;.See &quot;man sudo_root&quot; for details.  查看 &#x2F;etc&#x2F;...</div></div></div></a><a class="pagination-related" href="/cn/fcitx5_emoji_display/" title="Emoji 在 Fcitx5 提示栏正常显示的字体配置"><img class="cover" src="https://images.unsplash.com/photo-1646026371686-79950ceb6daa?w=640" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-12</div><div class="info-item-2">Emoji 在 Fcitx5 提示栏正常显示的字体配置</div></div><div class="info-2"><div class="info-item-1">关于升级大版本的 Linux 发行版，我有个经验，不要太着急升级，至少等2个星期再说。 即便如此，还是会遇上各种使用的问题， 升级 Fedora 41 后，Color Emoji 无法在 Fcitx5 的输入法提示栏正常显示。 上网搜索了一番，估计和字体渲染相关，下面是具体的解决问题方法：  安装 Emoji 字体  12sudo dnf install google-noto-color-emoji-fontssudo dnf install default-fonts-core-emoji   添加 color emoji 字体配置文件 /etc/fonts/conf.d/68-color-emoji.conf  123456789101112131415161718&lt;?xml version=&#x27;1.0&#x27;?&gt;&lt;!DOCTYPE fontconfig SYSTEM &#x27;fonts.dtd&#x27;&gt;&lt;fontconfig&gt;    &lt;match target=&quot;font&quot;&gt;     ...</div></div></div></a><a class="pagination-related" href="/cn/wps_fedora_linux_chinese_bad_char/" title="解决 Fedora Linux WPS 中文乱码问题"><img class="cover" src="https://images.unsplash.com/photo-1646026371686-79950ceb6daa?w=640" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-19</div><div class="info-item-2">解决 Fedora Linux WPS 中文乱码问题</div></div><div class="info-2"><div class="info-item-1">不知从哪个版本开始，Fedora Linux 上的 wps 字体乱码，基本处于不可用状态。最近，又遇上要打开 Office 文件的需求，研究了一下解决的方法。 安装缺少的字体，wps 可能没有某些字体的版权123456git clone https://github.com/jayknoxqu/wps-symbol-fontscd wps-symbol-fontssudo mkdir /usr/share/fonts/wps-fontssudo mv *.ttf /usr/share/fonts/wps-fontssudo chmod 644 /usr/share/fonts/wps-fonts/*sudo fc-cache -vfs  安装上面的字体，可以解决 wps 汉字显示为小方块的问题。 解决 wps 字体粗体显示不正常的问题wps 版本 11.1.0.11704-1 与 freetype2 版本 2.13.1 和 2.13.2 兼容性不佳。freetype2 会通过算法生成伪粗体（fakebold），但在这种情况下，wps-office 也自行将字体设为粗体，结果是文本...</div></div></div></a><a class="pagination-related" href="/cn/Fedora_linux_wechat_4.0/" title="Fedora linux 安装 wechat 4.0 的方法"><img class="cover" src="https://images.unsplash.com/photo-1646026371686-79950ceb6daa?w=640" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-05</div><div class="info-item-2">Fedora linux 安装 wechat 4.0 的方法</div></div><div class="info-2"><div class="info-item-1">感谢国产化软件项目，多年后，Linux 终于有 wechat 的官方安装包了 🥳 从麒麟 Linux 的源中，下载 wechat deb 安装包，解压后可以正常在 Fedora Linux 上使用。 123456wget https://archive2.kylinos.cn/DEB/KYLIN_DEB/pool/main/deb/wechat/wechat-beta_4.0.0.21_amd64.debsudo dnf install dpkg vlcdpkg-deb -xv ./wechat-beta_4.0.0.21_amd64.deb /tmp/wechatcd /tmp/wechat/opt/wechat-betasudo ln -sf /usr/lib64/libbz2.so.1.0.8 /usr/lib64/libbz2.so.1.0./wechat  我的 Fedora Linux 上安装了许多软件，常见的依赖库都已安装，大家可能还会遇到找不到动态库的情况。 其他 Linux 发行版 wechat 的适配原理上是一样的。 1）将 deb 包解开，执行 wecha...</div></div></div></a><a class="pagination-related" href="/cn/ssh-publickey-permission-denied/" title="ssh publickey permission denied 错误的解决方法"><img class="cover" src="https://images.unsplash.com/photo-1646026371686-79950ceb6daa?w=640" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-05</div><div class="info-item-2">ssh publickey permission denied 错误的解决方法</div></div><div class="info-2"><div class="info-item-1">openssh publickey 登录的方式突然不好用了，检查 grep sshd /var/log/auth.log 1Aug 30 16:58:33 pm-65c50001 sshd[9118]: Authentication refused: bad ownership or modes for directory /root  执行下面的命令后，可以成功登录了。 1234chown root:root /rootchown root:root /root/.sshchmod 700 /root/.sshchmod 600 /root/.ssh/authorized_keys  &#x2F;etc&#x2F;ssh&#x2F;sshd_config 文件中最关键的配置项 1PasswordAuthentication yes  我以前也遇到过这个问题，都是 file permession 设置错误，这次是 directory ownership 设置错误。排查此类错误，最靠谱的方法还是看日志，我折腾了 1 个小时。 Claude 3.5 Sonnet 没有回答出正确答案。...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/images/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">曼福吉</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">167</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">17</div></a><a href="/about"><div class="headline">关于</div></a></div><div class="card-info-social-icons"><a class="social-icon" href="/rss.xml" target="_blank" title="RSS"><i class="fas fa-rss" style="color: #4c566a;"></i></a><a class="social-icon" href="https://x.com/henices" target="_blank" title="Twitter"><i class="fab fa-x-twitter" style="color: #4c566a;"></i></a><a class="social-icon" href="mailto:zhouzhenster@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4c566a;"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85-Nvidia-%E9%A9%B1%E5%8A%A8"><span class="toc-number">1.</span> <span class="toc-text">安装 Nvidia 驱动</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85-ollama"><span class="toc-number">2.</span> <span class="toc-text">安装 ollama</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%87%E7%BA%A7-Ollama"><span class="toc-number">3.</span> <span class="toc-text">升级 Ollama</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE-Ollama"><span class="toc-number">4.</span> <span class="toc-text">配置 Ollama</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B8%E8%BD%BD-Ollama"><span class="toc-number">5.</span> <span class="toc-text">卸载 Ollama</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Ollama-%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">6.</span> <span class="toc-text">Ollama 的使用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%87%E6%8D%A2-Ollama-%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%96%87%E4%BB%B6%E4%BD%8D%E7%BD%AE"><span class="toc-number">6.1.</span> <span class="toc-text">切换 Ollama 的模型文件位置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A0%94%E7%A9%B6-Ollama-mutlple-GPU"><span class="toc-number">6.2.</span> <span class="toc-text">研究 Ollama mutlple GPU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%88%A4%E6%96%AD%E6%A8%A1%E5%9E%8B%E6%98%AF%E5%90%A6%E5%8A%A0%E8%BD%BD%E5%85%A5-GPU"><span class="toc-number">6.3.</span> <span class="toc-text">如何判断模型是否加载入 GPU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%85%E6%A0%B8%E8%AE%BE%E7%BD%AE-numa-balancing"><span class="toc-number">6.4.</span> <span class="toc-text">内核设置 numa_balancing</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">7.</span> <span class="toc-text">参考资料</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/cn/how_to_set_a_goal/" title="如何设定一个好目标">如何设定一个好目标</a><time datetime="2025-12-04T16:00:00.000Z" title="发表于 2025-12-05 00:00:00">2025-12-05</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/cn/know_Evolution/" title="如何理解演化论的思想">如何理解演化论的思想</a><time datetime="2025-11-20T16:00:00.000Z" title="发表于 2025-11-21 00:00:00">2025-11-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/cn/investment_expectation/" title="金融投资中的预期思维">金融投资中的预期思维</a><time datetime="2025-11-19T16:00:00.000Z" title="发表于 2025-11-20 00:00:00">2025-11-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/cn/duanyongping/" title="段永平退休20多年后罕见公开访谈">段永平退休20多年后罕见公开访谈</a><time datetime="2025-11-11T16:00:00.000Z" title="发表于 2025-11-12 00:00:00">2025-11-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/cn/how_to_tell_a_story/" title="如何讲好一个故事">如何讲好一个故事</a><time datetime="2025-11-11T16:00:00.000Z" title="发表于 2025-11-12 00:00:00">2025-11-12</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(https://images.unsplash.com/photo-1646026371686-79950ceb6daa?w=640);"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 By 曼福吉</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.2"></script><script src="/js/main.js?v=5.5.2"></script><script src="/js/tw_cn.js?v=5.5.2"></script><div class="js-pjax"><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const config = mermaidSrc.dataset.config ? JSON.parse(mermaidSrc.dataset.config) : {}
      if (!config.theme) {
        config.theme = theme
      }
      const mermaidThemeConfig = `%%{init: ${JSON.stringify(config)}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (false) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid@11.12.1/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><i class="fas fa-spinner fa-pulse" id="loading-status" hidden="hidden"></i><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="local-search-input"><input placeholder="搜索文章" type="text"/></div><hr/><div id="local-search-results"></div><div class="ais-Pagination" id="local-search-pagination" style="display:none;"><ul class="ais-Pagination-list"></ul></div><div id="local-search-stats"></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=5.5.2"></script></div></div></body></html>